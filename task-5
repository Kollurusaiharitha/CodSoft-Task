import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Load and preprocess the text dataset
dataset_path =r"C:\Haritha\archive\trumps loser rant.csv"  # Replace with the actual path to your text dataset


# Read the text data from the file with the correct encoding
with open(dataset_path, "r", encoding="ISO-8859-1") as file:  # Try ISO-8859-1 encoding
    text_data = file.read()


# Preprocessing: Clean and tokenize the text
import re
from tensorflow.keras.preprocessing.text import Tokenizer

# Clean the text by removing special characters, numbers, and extra spaces
cleaned_text = re.sub(r"[^a-zA-Z\s]", "", text_data).lower()
cleaned_text = re.sub(r"\s+", " ", cleaned_text).strip()


# Tokenize the cleaned text
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(cleaned_text)

# Convert characters to numerical sequences
sequences = tokenizer.texts_to_sequences(cleaned_text)

with open(r"C:\Haritha\archive\trumps loser rant.csv", 'r', encoding='ISO-8859-1') as f:
    text_data = f.read()


with open(r"C:\Haritha\archive\trumps loser rant.csv", 'r', encoding='utf-8', errors='ignore') as f:
    text_data = f.read()


# Create a vocabulary of unique characters
chars = sorted(list(set(text_data)))
char_to_idx = {char: idx for idx, char in enumerate(chars)}
idx_to_char = {idx: char for idx, char in enumerate(chars)}

# Convert text data to numerical sequences
seq_length = 100  # Length of input sequences
sequences = []
next_chars = []

for i in range(0, len(text_data) - seq_length, 1):
    seq = text_data[i:i + seq_length]
    next_char = text_data[i + seq_length]
    sequences.append([char_to_idx[char] for char in seq])
    next_chars.append(char_to_idx[next_char])


# Convert sequences to input tensor
X = np.reshape(sequences, (len(sequences), seq_length, 1))
X = X / float(len(chars))


# Convert next_chars to target tensor
y = tf.keras.utils.to_categorical(next_chars)


# Build the RNN model
model = Sequential([
    LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True),
    LSTM(256),
    Dense(y.shape[1], activation='softmax')
])


# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])


# Train the model
model.fit(X, y, batch_size=128, epochs=10)


# Generate text
start_idx = np.random.randint(0, len(sequences) - 1)
seed_text = sequences[start_idx]
generated_text = seed_text.copy()

for _ in range(400):  # Generate 400 characters
    x_pred = np.reshape(seed_text, (1, len(seed_text), 1))
    x_pred = x_pred / float(len(chars))
    pred_probs = model.predict(x_pred)[0]
    pred_idx = np.random.choice(len(chars), p=pred_probs)  # Sample the next character based on probabilities
    generated_text.append(pred_idx)
    seed_text.append(pred_idx)
    seed_text = seed_text[1:]
# Convert generated indices to characters
generated_text = [idx_to_char[idx] for idx in generated_text]
print(''.join(generated_text))

